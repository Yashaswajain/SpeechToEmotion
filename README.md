# SpeechToEmotion
<p>This project is based on the fact that our speech is vulnerable to many emotions</p>
<p>This project uses LIBROSA module for feature extraction from the dataset</p>
<p>It successfully determines the emotion from a person's tone and pitch of speaking with a maximum accuracy of 75.83%</p>
  
<h3>LIBRARIES USED</h3>
<ul>
  <li>LIBROSA:To analyze audio and sound files.</li>
  <li>SoundFile:To perform read and write operations on sound files.</li>
  <li>Pickle:To store the trained model.</li>
  <li>NumPy:To perform calculative operations on array.</li>
  <li>Sklearn:To use accuracy_score() function to determine accuracy of our model.</li>
  <li>Wave:To classify audio files(.wav specifically) as mono or stereo.</li>
  <li>Pydub:To perform transformations on audio files.</li>
</ul>
<h3>FEATURES OF SOUND USED</h3>
<ul>
  <li>mfcc:mel frequency cepstral coefficient->Represents short term power spectrum of a sound.</li>
  <li>chroma:to determine pitch out of 12 different pitches of sound.</li>
  <li>mel:mel spectrogram frequency</li>
</ul>
<h3>DATASET</h3>
<p>RAVDESS:Ryerson's Audio Visual Database of Emotional Speech and Song</p>
<h3>TECHNOLOGIES</h3>
<h4>LANGUAGES:</h4>
<ul>
  <li>Python</li>
  <li>Javascript</li>
  <li>PHP</li>
  <li>CSS</li>
  <li>HTML</li>
</ul>
<h4>FRAMEWORKS:</h4>
<ul>
  <li>Bootstrap</li>
</ul>

<h3>RESULTS:</h3>
![WhatsApp Image 2021-04-10 at 23 22 23 (1)](https://user-images.githubusercontent.com/44711728/115106431-6e4c6f80-9f82-11eb-8650-c1bee22d6a06.jpeg)
![WhatsApp Image 2021-04-10 at 23 22 23](https://user-images.githubusercontent.com/44711728/115106432-70aec980-9f82-11eb-814b-fd1bb7eed4cc.jpeg)
![on](https://user-images.githubusercontent.com/44711728/115106435-74dae700-9f82-11eb-9d6e-59eedb1e43e3.PNG)
![text](https://user-images.githubusercontent.com/44711728/115106436-76a4aa80-9f82-11eb-80f3-fa7bc02f34c4.PNG)











